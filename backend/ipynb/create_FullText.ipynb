{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection merger completed successfully! ðŸŽ‰\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import voyageai\n",
    "\n",
    "# Load those environment variables (because who doesn't love a good secret?)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Base class for embedding generation using Voyage AI\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client = voyageai.Client(\n",
    "            api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
    "        )\n",
    "        self.batch_size = 128  # Voyage's maximum batch size\n",
    "    \n",
    "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for multiple texts using Voyage AI\"\"\"\n",
    "        try:\n",
    "            response = self.client.embed(\n",
    "                texts=texts,\n",
    "                model=\"voyage-3\",\n",
    "                input_type=\"document\",\n",
    "                output_dimension=1024\n",
    "            )\n",
    "            return response.embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in Voyage AI batch embedding generation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"Legacy method for single text embedding\"\"\"\n",
    "        embeddings = await self.embed_batch([text])\n",
    "        return embeddings[0]\n",
    "\n",
    "    async def embed_many(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Process a large number of texts in optimal batches\"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = await self.embed_batch(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "class QdrantMerger:\n",
    "    def __init__(self):\n",
    "        self.client = QdrantClient(\n",
    "            url=os.getenv(\"QDRANT_URL\"),\n",
    "            api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    "        )\n",
    "        self.embedding_model = EmbeddingModel()\n",
    "        \n",
    "    async def fetch_collection_data(self, collection_name: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Fetch all points from a collection and index them by filename\"\"\"\n",
    "        result = {}\n",
    "        offset = None\n",
    "        duplicates = set()  # To keep track of our overeager duplicates\n",
    "        \n",
    "        while True:\n",
    "            batch, offset = self.client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                with_payload=True,\n",
    "                with_vectors=False,\n",
    "                offset=offset\n",
    "            )\n",
    "            \n",
    "            # Process each point in the batch\n",
    "            for point in batch:\n",
    "                filename = point.payload.get('filename')\n",
    "                if filename:\n",
    "                    if filename in result:\n",
    "                        # Oops, looks like we found a duplicate!\n",
    "                        duplicates.add(filename)\n",
    "                    result[filename] = point.payload\n",
    "            \n",
    "            # If no more offset, we've reached the end of our dating pool\n",
    "            if offset is None:\n",
    "                break\n",
    "        \n",
    "        # Let's be responsible and warn about any duplicate profiles we found\n",
    "        if duplicates:\n",
    "            logging.warning(f\"Found duplicate entries for filenames: {duplicates}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def create_merged_collection(self):\n",
    "        \"\"\"Create and populate the merged collection\"\"\"\n",
    "        # First, let's get our lonely hearts from both collections\n",
    "        personality_data = await self.fetch_collection_data(\"Personality\")\n",
    "        storyteller_data = await self.fetch_collection_data(\"Storyteller\")\n",
    "        \n",
    "        # Create our new love nest (collection)\n",
    "        self.client.recreate_collection(\n",
    "            collection_name=\"Full_Texts\",\n",
    "            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)\n",
    "        )\n",
    "        \n",
    "        # Time to play matchmaker!\n",
    "        merged_points = []\n",
    "        for filename in personality_data.keys():\n",
    "            if filename in storyteller_data:\n",
    "                # Found a match! Let's bring them together\n",
    "                merged_payload = {\n",
    "                    \"filename\": filename,\n",
    "                    \"raw_text\": personality_data[filename][\"raw_text\"],\n",
    "                    \"personality\": personality_data[filename][\"personality\"],\n",
    "                    \"story\": storyteller_data[filename][\"story\"]\n",
    "                }\n",
    "                \n",
    "                # Generate new embedding for the combined text\n",
    "                combined_text = \"\\n\\n\".join([\n",
    "                    f\"ðŸ“„ RESUME:\\n{merged_payload['raw_text']}\",\n",
    "                    f\"ðŸ§  PERSONALITY:\\n{merged_payload['personality']}\",\n",
    "                    f\"ðŸ“š STORY:\\n{merged_payload['story']}\"\n",
    "                ])\n",
    "                embedding = await self.embedding_model.embed_many([combined_text])\n",
    "                \n",
    "                # Create the happy couple (point)\n",
    "                merged_points.append(PointStruct(\n",
    "                    id=hash(filename),  # Using filename hash as ID\n",
    "                    payload=merged_payload,\n",
    "                    vector=embedding[0]\n",
    "                ))\n",
    "        \n",
    "        # Upload our happy couples in batches (because even Cupid needs breaks)\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(merged_points), batch_size):\n",
    "            batch = merged_points[i:i + batch_size]\n",
    "            self.client.upsert(\n",
    "                collection_name=\"Full_Texts\",\n",
    "                points=batch\n",
    "            )\n",
    "            logging.info(f\"Uploaded batch {i//batch_size + 1} of {(len(merged_points)-1)//batch_size + 1}\")\n",
    "\n",
    "async def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    merger = QdrantMerger()\n",
    "    await merger.create_merged_collection()\n",
    "    logging.info(\"Collection merger completed successfully! ðŸŽ‰\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/personality/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/storyteller/points/scroll \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_26384\\2390138011.py:96: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  self.client.recreate_collection(\n",
      "INFO:httpx:HTTP Request: DELETE https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Processed chunk 1 of 4\n",
      "INFO:root:Processed chunk 2 of 4\n",
      "INFO:root:Processed chunk 3 of 4\n",
      "INFO:root:Processed chunk 4 of 4\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Uploaded batch 1 of 5\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Uploaded batch 2 of 5\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Uploaded batch 3 of 5\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Uploaded batch 4 of 5\n",
      "INFO:httpx:HTTP Request: PUT https://daf5172e-cb38-4794-9a1e-935ac3be2a22.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Full_Texts/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Uploaded batch 5 of 5\n",
      "INFO:root:Collection merger completed successfully! ðŸŽ‰ Time to delete the evidence!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "import asyncio\n",
    "from hashlib import md5\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import voyageai\n",
    "\n",
    "load_dotenv()  # Loading secrets like they're your DMs\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Base class for embedding generation using Voyage AI\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client = voyageai.Client(\n",
    "            api_key=os.getenv(\"VOYAGE_API_KEY\")\n",
    "        )\n",
    "        self.batch_size = 128  # Voyage's party size limit\n",
    "    \n",
    "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts using Voyage AI\"\"\"\n",
    "        try:\n",
    "            response = self.client.embed(\n",
    "                texts=texts,\n",
    "                model=\"voyage-3\",\n",
    "                input_type=\"document\",\n",
    "                output_dimension=1024\n",
    "            )\n",
    "            return response.embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Voyage AI said 'no' with error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class QdrantMerger:\n",
    "    def __init__(self):\n",
    "        self.client = QdrantClient(\n",
    "            url=os.getenv(\"QDRANT_URL\"),\n",
    "            api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    "        )\n",
    "        self.embedding_model = EmbeddingModel()\n",
    "        \n",
    "    async def fetch_collection_data(self, collection_name: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Fetch all points from a collection and index them by filename\"\"\"\n",
    "        result = {}\n",
    "        offset = None\n",
    "        duplicates = set()\n",
    "        \n",
    "        while True:\n",
    "            batch, offset = self.client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                with_payload=True,\n",
    "                with_vectors=False,\n",
    "                offset=offset\n",
    "            )\n",
    "            \n",
    "            for point in batch:\n",
    "                filename = point.payload.get('filename')\n",
    "                if filename:\n",
    "                    if filename in result:\n",
    "                        duplicates.add(filename)\n",
    "                    result[filename] = point.payload\n",
    "            \n",
    "            if offset is None:\n",
    "                break\n",
    "        \n",
    "        if duplicates:\n",
    "            logging.warning(f\"Found some duplicate attention seekers: {duplicates}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _prepare_text(self, raw_text: str, personality: str, story: str) -> str:\n",
    "        \"\"\"Combine texts with formatting that would make your English teacher proud\"\"\"\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"ðŸ“„ RESUME:\\n{raw_text}\",\n",
    "            f\"ðŸ§  PERSONALITY:\\n{personality}\",\n",
    "            f\"ðŸ“š STORY:\\n{story}\"\n",
    "        ])\n",
    "\n",
    "    def _create_point(self, filename: str, payload: Dict, embedding: List[float]) -> PointStruct:\n",
    "        \"\"\"Create a point that's more stable than your ex's promises\"\"\"\n",
    "        return PointStruct(\n",
    "            id=int(md5(filename.encode()).hexdigest()[:16], 16),\n",
    "            payload=payload,\n",
    "            vector=embedding\n",
    "        )\n",
    "    \n",
    "    async def create_merged_collection(self):\n",
    "        \"\"\"Create and populate the merged collection, like a dating app but for data\"\"\"\n",
    "        # Fetch the lonely data points looking for love\n",
    "        personality_data = await self.fetch_collection_data(\"personality\")\n",
    "        storyteller_data = await self.fetch_collection_data(\"storyteller\")\n",
    "        \n",
    "        # Create our new collection (swipe right to match!)\n",
    "        self.client.recreate_collection(\n",
    "            collection_name=\"Full_Texts\",\n",
    "            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)\n",
    "        )\n",
    "        \n",
    "        # Prepare all our potential matches\n",
    "        texts_to_embed = []\n",
    "        file_order = []\n",
    "        payloads = []\n",
    "        \n",
    "        for filename in personality_data:\n",
    "            if filename in storyteller_data:  # If it's a match!\n",
    "                payload = {\n",
    "                    \"filename\": filename,\n",
    "                    \"raw_text\": personality_data[filename][\"raw_text\"],\n",
    "                    \"personality\": personality_data[filename][\"personality\"],\n",
    "                    \"story\": storyteller_data[filename][\"story\"]\n",
    "                }\n",
    "                payloads.append(payload)\n",
    "                file_order.append(filename)\n",
    "                texts_to_embed.append(self._prepare_text(\n",
    "                    payload[\"raw_text\"],\n",
    "                    payload[\"personality\"],\n",
    "                    payload[\"story\"]\n",
    "                ))\n",
    "        \n",
    "        # Process embeddings in bite-sized chunks (because nobody likes a choking hazard)\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts_to_embed), self.embedding_model.batch_size):\n",
    "            chunk = texts_to_embed[i:i + self.embedding_model.batch_size]\n",
    "            embeddings = await self.embedding_model.embed_batch(chunk)\n",
    "            all_embeddings.extend(embeddings)\n",
    "            logging.info(f\"Processed chunk {i//self.embedding_model.batch_size + 1} of \"\n",
    "                        f\"{(len(texts_to_embed) + self.embedding_model.batch_size - 1)//self.embedding_model.batch_size}\")\n",
    "        \n",
    "        # Create our happy couples\n",
    "        points = [\n",
    "            self._create_point(filename, payload, embedding)\n",
    "            for filename, payload, embedding in zip(file_order, payloads, all_embeddings)\n",
    "        ]\n",
    "        \n",
    "        # Upload in batches (because even Cupid needs a break)\n",
    "        upload_batch_size = 100\n",
    "        for i in range(0, len(points), upload_batch_size):\n",
    "            batch = points[i:i + upload_batch_size]\n",
    "            self.client.upsert(collection_name=\"Full_Texts\", points=batch)\n",
    "            logging.info(f\"Uploaded batch {i//upload_batch_size + 1} of {(len(points)-1)//upload_batch_size + 1}\")\n",
    "\n",
    "async def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    merger = QdrantMerger()\n",
    "    await merger.create_merged_collection()\n",
    "    logging.info(\"Collection merger completed successfully! ðŸŽ‰ Time to delete the evidence!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
